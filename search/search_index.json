{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Enabling Smarter Workloads: AI on Z \u00b6","title":"Welcome"},{"location":"#enabling-smarter-workloads-ai-on-z","text":"","title":"Enabling Smarter Workloads: AI on Z"},{"location":"ai-on-z-containers/","text":"ai-on-z-containers \u00b6 Scope \u00b6 The purpose of this project is to provide container build files for AI software that can be utilized in s390x environments. These container files (i.e., dockerfiles or containerfiles) are provided as examples that can be used directly or built upon. They build open-source based (not proprietary) images. Usage \u00b6 These build files commonly rely on base images from the IBM Z and LinuxONE Container Image Registry (ICR) . This will require free basic authentication. Details can be found at the ICR link above. Additionally, note that numerous pre-built s390x images are available in ICR. Content \u00b6 Folder(topic) Description nlp-spaCy spaCy library for natural language processing use cases License \u00b6 If you would like to see the detailed LICENSE click here . # # Copyright 2020- IBM Inc. All rights reserved # SPDX-License-Identifier: Apache2.0 #","title":"Containers"},{"location":"ai-on-z-containers/#ai-on-z-containers","text":"","title":"ai-on-z-containers"},{"location":"ai-on-z-containers/#scope","text":"The purpose of this project is to provide container build files for AI software that can be utilized in s390x environments. These container files (i.e., dockerfiles or containerfiles) are provided as examples that can be used directly or built upon. They build open-source based (not proprietary) images.","title":"Scope"},{"location":"ai-on-z-containers/#usage","text":"These build files commonly rely on base images from the IBM Z and LinuxONE Container Image Registry (ICR) . This will require free basic authentication. Details can be found at the ICR link above. Additionally, note that numerous pre-built s390x images are available in ICR.","title":"Usage"},{"location":"ai-on-z-containers/#content","text":"Folder(topic) Description nlp-spaCy spaCy library for natural language processing use cases","title":"Content"},{"location":"ai-on-z-containers/#license","text":"If you would like to see the detailed LICENSE click here . # # Copyright 2020- IBM Inc. All rights reserved # SPDX-License-Identifier: Apache2.0 #","title":"License"},{"location":"ai-on-z-containers/CONTRIBUTING/","text":"Contributing In General \u00b6 Our project welcomes external contributions. If you have an itch, please feel free to scratch it. To contribute code or documentation, please submit a FIXME pull request . A good way to familiarize yourself with the codebase and contribution process is to look for and tackle low-hanging fruit in the FIXME issue tracker . Before embarking on a more ambitious contribution, please quickly get in touch with us. Note: We appreciate your effort, and want to avoid a situation where a contribution requires extensive rework (by you or by us), sits in backlog for a long time, or cannot be accepted at all! Proposing new features \u00b6 If you would like to implement a new feature, please FIXME raise an issue before sending a pull request so the feature can be discussed. This is to avoid you wasting your valuable time working on a feature that the project developers are not interested in accepting into the code base. Fixing bugs \u00b6 If you would like to fix a bug, please FIXME raise an issue before sending a pull request so it can be tracked. Merge approval \u00b6 The project maintainers use LGTM (Looks Good To Me) in comments on the code review to indicate acceptance. A change requires LGTMs from two of the maintainers of each component affected. For a list of the maintainers, see the MAINTAINERS.md page. Legal \u00b6 Each source file must include a license header for the Apache Software License 2.0. Using the SPDX format is the simplest approach. e.g. /* Copyright <holder> All Rights Reserved. SPDX-License-Identifier: Apache-2.0 */ We have tried to make it as easy as possible to make contributions. This applies to how we handle the legal aspects of contribution. We use the same approach - the Developer's Certificate of Origin 1.1 (DCO) - that the Linux\u00ae Kernel community uses to manage code contributions. We simply ask that when submitting a patch for review, the developer must include a sign-off statement in the commit message. Here is an example Signed-off-by line, which indicates that the submitter accepts the DCO: Signed-off-by: John Doe <john.doe@example.com> You can include this automatically when you commit a change to your local git repository using the following command: git commit -s Communication \u00b6 FIXME Please feel free to connect with us on our Slack channel . Setup \u00b6 FIXME Please add any special setup instructions for your project to help the developer become productive quickly. Testing \u00b6 FIXME Please provide information that helps the developer test any changes they make before submitting. Coding style guidelines \u00b6 FIXME Optional, but recommended: please share any specific style guidelines you might have for your project.","title":"CONTRIBUTING"},{"location":"ai-on-z-containers/CONTRIBUTING/#contributing-in-general","text":"Our project welcomes external contributions. If you have an itch, please feel free to scratch it. To contribute code or documentation, please submit a FIXME pull request . A good way to familiarize yourself with the codebase and contribution process is to look for and tackle low-hanging fruit in the FIXME issue tracker . Before embarking on a more ambitious contribution, please quickly get in touch with us. Note: We appreciate your effort, and want to avoid a situation where a contribution requires extensive rework (by you or by us), sits in backlog for a long time, or cannot be accepted at all!","title":"Contributing In General"},{"location":"ai-on-z-containers/CONTRIBUTING/#proposing-new-features","text":"If you would like to implement a new feature, please FIXME raise an issue before sending a pull request so the feature can be discussed. This is to avoid you wasting your valuable time working on a feature that the project developers are not interested in accepting into the code base.","title":"Proposing new features"},{"location":"ai-on-z-containers/CONTRIBUTING/#fixing-bugs","text":"If you would like to fix a bug, please FIXME raise an issue before sending a pull request so it can be tracked.","title":"Fixing bugs"},{"location":"ai-on-z-containers/CONTRIBUTING/#merge-approval","text":"The project maintainers use LGTM (Looks Good To Me) in comments on the code review to indicate acceptance. A change requires LGTMs from two of the maintainers of each component affected. For a list of the maintainers, see the MAINTAINERS.md page.","title":"Merge approval"},{"location":"ai-on-z-containers/CONTRIBUTING/#legal","text":"Each source file must include a license header for the Apache Software License 2.0. Using the SPDX format is the simplest approach. e.g. /* Copyright <holder> All Rights Reserved. SPDX-License-Identifier: Apache-2.0 */ We have tried to make it as easy as possible to make contributions. This applies to how we handle the legal aspects of contribution. We use the same approach - the Developer's Certificate of Origin 1.1 (DCO) - that the Linux\u00ae Kernel community uses to manage code contributions. We simply ask that when submitting a patch for review, the developer must include a sign-off statement in the commit message. Here is an example Signed-off-by line, which indicates that the submitter accepts the DCO: Signed-off-by: John Doe <john.doe@example.com> You can include this automatically when you commit a change to your local git repository using the following command: git commit -s","title":"Legal"},{"location":"ai-on-z-containers/CONTRIBUTING/#communication","text":"FIXME Please feel free to connect with us on our Slack channel .","title":"Communication"},{"location":"ai-on-z-containers/CONTRIBUTING/#setup","text":"FIXME Please add any special setup instructions for your project to help the developer become productive quickly.","title":"Setup"},{"location":"ai-on-z-containers/CONTRIBUTING/#testing","text":"FIXME Please provide information that helps the developer test any changes they make before submitting.","title":"Testing"},{"location":"ai-on-z-containers/CONTRIBUTING/#coding-style-guidelines","text":"FIXME Optional, but recommended: please share any specific style guidelines you might have for your project.","title":"Coding style guidelines"},{"location":"ai-on-z-containers/MAINTAINERS/","text":"MAINTAINERS \u00b6 Christopher Ferris - chrisfer@us.ibm.com","title":"MAINTAINERS"},{"location":"ai-on-z-containers/MAINTAINERS/#maintainers","text":"Christopher Ferris - chrisfer@us.ibm.com","title":"MAINTAINERS"},{"location":"ai-on-z-containers/nlp-spaCy/","text":"spaCy, a natural language processing library. \u00b6 Overview \u00b6 spaCy.io spaCy github from the spaCy github project: spaCy is a library for advanced Natural Language Processing in Python and Cython. It's built on the very latest research, and was designed from day one to be used in real products. spaCy comes with pretrained pipelines and currently supports tokenization and training for 60+ languages. It features state-of-the-art speed and neural network models for tagging, parsing, named entity recognition, text classification and more, multi-task learning with pretrained transformers like BERT, as well as a production-ready training system and easy model packaging, deployment and workflow management. spaCy is commercial open-source software, released under the MIT license. Usage \u00b6 This image can be built using the podman or docker build command. e.g., docker build -t 'spacy_s390x' . Note that this docker files relies on base images from the IBM Z and LinuxONE Container Image Registry (ICR) . This build file includes jupyter and an example notebook to demonstrate basic syntax analysis with spaCy. Once this image is built, you can start a container using a command like this: docker run -d --rm -p 8571:8888 <image_id> This will start a container which exposing jupyter port 8888 on host system port 8571. If you specify -d (as above) you will need to display the jupyter token; this can be found by issuing docker logs <container_id> You can then direct your web browser to <host_ip_addr>:<exposed_port>/?token=<jupyter_token>","title":"Index"},{"location":"ai-on-z-containers/nlp-spaCy/#spacy-a-natural-language-processing-library","text":"","title":"spaCy, a natural language processing library."},{"location":"ai-on-z-containers/nlp-spaCy/#overview","text":"spaCy.io spaCy github from the spaCy github project: spaCy is a library for advanced Natural Language Processing in Python and Cython. It's built on the very latest research, and was designed from day one to be used in real products. spaCy comes with pretrained pipelines and currently supports tokenization and training for 60+ languages. It features state-of-the-art speed and neural network models for tagging, parsing, named entity recognition, text classification and more, multi-task learning with pretrained transformers like BERT, as well as a production-ready training system and easy model packaging, deployment and workflow management. spaCy is commercial open-source software, released under the MIT license.","title":"Overview"},{"location":"ai-on-z-containers/nlp-spaCy/#usage","text":"This image can be built using the podman or docker build command. e.g., docker build -t 'spacy_s390x' . Note that this docker files relies on base images from the IBM Z and LinuxONE Container Image Registry (ICR) . This build file includes jupyter and an example notebook to demonstrate basic syntax analysis with spaCy. Once this image is built, you can start a container using a command like this: docker run -d --rm -p 8571:8888 <image_id> This will start a container which exposing jupyter port 8888 on host system port 8571. If you specify -d (as above) you will need to display the jupyter token; this can be found by issuing docker logs <container_id> You can then direct your web browser to <host_ip_addr>:<exposed_port>/?token=<jupyter_token>","title":"Usage"},{"location":"ai-on-z-fraud-detection/","text":"The dataset used in this repo can be found here: https://github.com/IBM/TabFormer/tree/main/data/credit_card \u00b6 Scope \u00b6 This repository provides TensorFlow source code for building and training credit card fraud models using an LSTM and a GRU. The saved models were converted to the ONNX format and are also included. License \u00b6 If you would like to see the detailed LICENSE click here .","title":"Fraud Detection"},{"location":"ai-on-z-fraud-detection/#the-dataset-used-in-this-repo-can-be-found-here-httpsgithubcomibmtabformertreemaindatacredit_card","text":"","title":"The dataset used in this repo can be found here:  https://github.com/IBM/TabFormer/tree/main/data/credit_card"},{"location":"ai-on-z-fraud-detection/#scope","text":"This repository provides TensorFlow source code for building and training credit card fraud models using an LSTM and a GRU. The saved models were converted to the ONNX format and are also included.","title":"Scope"},{"location":"ai-on-z-fraud-detection/#license","text":"If you would like to see the detailed LICENSE click here .","title":"License"},{"location":"ai-on-z-samples/","text":"ai-on-z-samples \u00b6 Scope \u00b6 This repository contains small, useful examples that serve to demonstrate some of the interesting technologies available for use on IBM Z and LinuxONE systems. Samples in this project: \u00b6 TensorFlow Serving on zCX and z/OS simple app example. ONNX model export and conversion examples Notes \u00b6 If you have any questions or issues you can create a new [issue here][issues]. If you have created a sample that you would like to share, please do so - pull requests are very welcome! Ideally create a topic branch for every contribution or change you would like to suggest. For example: Fork the repo Create your feature branch ( git checkout -b my-new-feature ) Commit your changes ( git commit -am 'Added some feature' ) Push to the branch ( git push origin my-new-feature ) Create new Pull Request License \u00b6 If you would like to see the detailed LICENSE click here . # # Copyright 2020- IBM Inc. All rights reserved # SPDX-License-Identifier: Apache2.0 #","title":"Samples"},{"location":"ai-on-z-samples/#ai-on-z-samples","text":"","title":"ai-on-z-samples"},{"location":"ai-on-z-samples/#scope","text":"This repository contains small, useful examples that serve to demonstrate some of the interesting technologies available for use on IBM Z and LinuxONE systems.","title":"Scope"},{"location":"ai-on-z-samples/#samples-in-this-project","text":"TensorFlow Serving on zCX and z/OS simple app example. ONNX model export and conversion examples","title":"Samples in this project:"},{"location":"ai-on-z-samples/#notes","text":"If you have any questions or issues you can create a new [issue here][issues]. If you have created a sample that you would like to share, please do so - pull requests are very welcome! Ideally create a topic branch for every contribution or change you would like to suggest. For example: Fork the repo Create your feature branch ( git checkout -b my-new-feature ) Commit your changes ( git commit -am 'Added some feature' ) Push to the branch ( git push origin my-new-feature ) Create new Pull Request","title":"Notes"},{"location":"ai-on-z-samples/#license","text":"If you would like to see the detailed LICENSE click here . # # Copyright 2020- IBM Inc. All rights reserved # SPDX-License-Identifier: Apache2.0 #","title":"License"},{"location":"ai-on-z-samples/CONTRIBUTING/","text":"Contributing In General \u00b6 Our project welcomes external contributions. If you have an itch, please feel free to scratch it. To contribute code or documentation, please submit a pull request . A good way to familiarize yourself with the codebase and contribution process is to look for and tackle low-hanging fruit in the issue tracker . /* Copyright <holder> All Rights Reserved. SPDX-License-Identifier: Apache-2.0 */","title":"CONTRIBUTING"},{"location":"ai-on-z-samples/CONTRIBUTING/#contributing-in-general","text":"Our project welcomes external contributions. If you have an itch, please feel free to scratch it. To contribute code or documentation, please submit a pull request . A good way to familiarize yourself with the codebase and contribution process is to look for and tackle low-hanging fruit in the issue tracker . /* Copyright <holder> All Rights Reserved. SPDX-License-Identifier: Apache-2.0 */","title":"Contributing In General"},{"location":"ai-on-z-samples/MAINTAINERS/","text":"MAINTAINERS \u00b6 Andrew M. Sica - andrewsi@us.ibm.com Steve Lafalce - slafalce@us.ibm.com","title":"MAINTAINERS"},{"location":"ai-on-z-samples/MAINTAINERS/#maintainers","text":"Andrew M. Sica - andrewsi@us.ibm.com Steve Lafalce - slafalce@us.ibm.com","title":"MAINTAINERS"},{"location":"ai-on-z-samples/onnx-conversion/","text":"Exporting or converting a model to the ONNX format \u00b6 This project contains some simple jupyter notebook examples demonstrating ONNX model conversion. This includes: - Exporting a simple model from Pytorch to ONNX format. - Converting a tensorflow model to ONNX, using tensorflow-onnx . These examples will require you install needed packages. This is not done as part of the notebook. There are numerous additional examples and guidance available, not only for Pytorch and TensorFlow, but for other frameworks as well. This includes: - Building, exporting or converting a model here - tensorflow-onnx examples - pytorch guidance and examples On IBM Z and LinuxONE, you can run these models using ONNX-MLIR . For z/OS users, we recommend you try Watson Machine Learning for z/OS Trial edition, available here .","title":"Exporting or converting a model to the ONNX format"},{"location":"ai-on-z-samples/onnx-conversion/#exporting-or-converting-a-model-to-the-onnx-format","text":"This project contains some simple jupyter notebook examples demonstrating ONNX model conversion. This includes: - Exporting a simple model from Pytorch to ONNX format. - Converting a tensorflow model to ONNX, using tensorflow-onnx . These examples will require you install needed packages. This is not done as part of the notebook. There are numerous additional examples and guidance available, not only for Pytorch and TensorFlow, but for other frameworks as well. This includes: - Building, exporting or converting a model here - tensorflow-onnx examples - pytorch guidance and examples On IBM Z and LinuxONE, you can run these models using ONNX-MLIR . For z/OS users, we recommend you try Watson Machine Learning for z/OS Trial edition, available here .","title":"Exporting or converting a model to the ONNX format"},{"location":"ai-on-z-samples/tf-zcx-zos/","text":"Demonstrating interaction between z/OS and TensorFlow Serving \u00b6 This project intends to demonstrate a call to TensorFlow Serving using REST API from a simple z/OS Java app. The purpose is to create a simple and easily deployable scenario that can be used on z/OS to understand serving concepts. This model performns a matrix multiplication of a [1,5] input tensor by a [5,1] weights tensor, where all weights are defined with a value of 1. This results in an output tensor shape of [1,1]. For example, with this model an input tensor of [[1,2,3,4,5]] is multiplied by a weights tensor of [[1],[1],[1],[1],[1]], resulting in a value of [[15.]] Note that since this project is intended for deployment to z/OS, we are avoiding managing dependecies through sub-moduling or Maven/Ant. The intent is to quickly try this project out without installing additional software. Required Jar files can be pulled from the references below. Deploying TensorFlow Serving on z/OS Container Extensions \u00b6 TensorFlow Serving can be obtained from the IBM Container Image Repository or built from source for Linux on Z IBM Container Image Repository Source Details on the TFServing API can be found here: https://www.tensorflow.org/tfx/serving/api_rest To deploy the example model, you can follow this procedure: SFTP the TensorFlow saved model to zCX. This should be the model folder and all subfolders. The saved model can be found in the model subdirectory of this project. The notebook used to create the model is simplemm.ipynb Create a new docker volume. e.g., docker volume create tfmodels tfmodels is the volume name we create to use in subsequent steps Copy the model directory into the docker volume. One approach is to create a simple container using the volume to allow a docker cp: docker container create --name tfsimple -v tfmodels:/models simple_image docker cp <host_model_dir> tfsimple:/models tfsimple is the container name we create to facilitate the copy via docker CP. simple_image can be any base image, and it can be deleted after this copy. models is a directory we choose to copy the model into. Run the TFServing image: - docker run -d --rm -p 8507:8501 -v tfmodels:/models -e MODEL_NAME=<model_name> <image id> - 8501 is the default TFServing REST port. Here it is mapped to zCX port 8507. - model_name is the TensorFlow model name; this is commonly the directory name that holds the saved_model.pb file Updating and deploying the z/OS application \u00b6 First, deploy the project to your host system. This program is intended for Unix environments. Suggestion on how to get github projects to z/OS can be found here: https://github.com/IBM/IBM-Z-zOS The following Jar files must be in the project root directory; Apache Commons Logging: commons-logging-1.1.3.jar Apache HttpClient: httpclient-4.5.13.jar Apache Httpcore: httpcore-4.4.14.jar Json Processing API javax.json-1.0.jar We suggest using SFTP to transmit the jars to the z/OS environment. Note: If different versions are used, update the Makefile to reference the correect version. run make to build compile the .java files issue the following command to try a TensorFlow Serving request: make run HOSTIP=tf-serve-ip PORT=tf-serv-port MODEL_DIR=model-dir PAYLOAD=input-tensor tf-serve-ip is the IP address of the server or instance hosting TensorFlow Serving tf-serv-port is the TF Serving REST port model-dir is the model path, which should typically be /v1/models/simplemm for the sample model. input-tensor consists of 5 comma separated decimal values. For example, if we mapped the TFServing REST port to 8507, we would use something like this (changing the IP address to one in use by zCX): make run HOSTIP=127.0.0.1 PORT=8507 MODEL_DIR=/v1/models/simplemm PAYLOAD=1.0,2.0,3.0,4.0,5.0 In addition to verbose messages, the result for test would show: { \"predictions\": [[15.0] ] }","title":"Demonstrating interaction between z/OS and TensorFlow Serving"},{"location":"ai-on-z-samples/tf-zcx-zos/#demonstrating-interaction-between-zos-and-tensorflow-serving","text":"This project intends to demonstrate a call to TensorFlow Serving using REST API from a simple z/OS Java app. The purpose is to create a simple and easily deployable scenario that can be used on z/OS to understand serving concepts. This model performns a matrix multiplication of a [1,5] input tensor by a [5,1] weights tensor, where all weights are defined with a value of 1. This results in an output tensor shape of [1,1]. For example, with this model an input tensor of [[1,2,3,4,5]] is multiplied by a weights tensor of [[1],[1],[1],[1],[1]], resulting in a value of [[15.]] Note that since this project is intended for deployment to z/OS, we are avoiding managing dependecies through sub-moduling or Maven/Ant. The intent is to quickly try this project out without installing additional software. Required Jar files can be pulled from the references below.","title":"Demonstrating interaction between z/OS and TensorFlow Serving"},{"location":"ai-on-z-samples/tf-zcx-zos/#deploying-tensorflow-serving-on-zos-container-extensions","text":"TensorFlow Serving can be obtained from the IBM Container Image Repository or built from source for Linux on Z IBM Container Image Repository Source Details on the TFServing API can be found here: https://www.tensorflow.org/tfx/serving/api_rest To deploy the example model, you can follow this procedure: SFTP the TensorFlow saved model to zCX. This should be the model folder and all subfolders. The saved model can be found in the model subdirectory of this project. The notebook used to create the model is simplemm.ipynb Create a new docker volume. e.g., docker volume create tfmodels tfmodels is the volume name we create to use in subsequent steps Copy the model directory into the docker volume. One approach is to create a simple container using the volume to allow a docker cp: docker container create --name tfsimple -v tfmodels:/models simple_image docker cp <host_model_dir> tfsimple:/models tfsimple is the container name we create to facilitate the copy via docker CP. simple_image can be any base image, and it can be deleted after this copy. models is a directory we choose to copy the model into. Run the TFServing image: - docker run -d --rm -p 8507:8501 -v tfmodels:/models -e MODEL_NAME=<model_name> <image id> - 8501 is the default TFServing REST port. Here it is mapped to zCX port 8507. - model_name is the TensorFlow model name; this is commonly the directory name that holds the saved_model.pb file","title":"Deploying TensorFlow Serving on z/OS Container Extensions"},{"location":"ai-on-z-samples/tf-zcx-zos/#updating-and-deploying-the-zos-application","text":"First, deploy the project to your host system. This program is intended for Unix environments. Suggestion on how to get github projects to z/OS can be found here: https://github.com/IBM/IBM-Z-zOS The following Jar files must be in the project root directory; Apache Commons Logging: commons-logging-1.1.3.jar Apache HttpClient: httpclient-4.5.13.jar Apache Httpcore: httpcore-4.4.14.jar Json Processing API javax.json-1.0.jar We suggest using SFTP to transmit the jars to the z/OS environment. Note: If different versions are used, update the Makefile to reference the correect version. run make to build compile the .java files issue the following command to try a TensorFlow Serving request: make run HOSTIP=tf-serve-ip PORT=tf-serv-port MODEL_DIR=model-dir PAYLOAD=input-tensor tf-serve-ip is the IP address of the server or instance hosting TensorFlow Serving tf-serv-port is the TF Serving REST port model-dir is the model path, which should typically be /v1/models/simplemm for the sample model. input-tensor consists of 5 comma separated decimal values. For example, if we mapped the TFServing REST port to 8507, we would use something like this (changing the IP address to one in use by zCX): make run HOSTIP=127.0.0.1 PORT=8507 MODEL_DIR=/v1/models/simplemm PAYLOAD=1.0,2.0,3.0,4.0,5.0 In addition to verbose messages, the result for test would show: { \"predictions\": [[15.0] ] }","title":"Updating and deploying the z/OS application"}]}