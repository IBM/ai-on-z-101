{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Enabling Smarter Workloads: AI on Z \u00b6","title":"Welcome"},{"location":"#enabling-smarter-workloads-ai-on-z","text":"","title":"Enabling Smarter Workloads: AI on Z"},{"location":"ai-on-z-samples/","text":"ai-on-z-samples \u00b6 Scope \u00b6 This repository contains small, useful examples that serve to demonstrate some of the interesting technologies available for use on IBM Z and LinuxONE systems. Samples in this project: \u00b6 TensorFlow Serving on zCX and z/OS simple app example. ONNX model export and conversion examples Notes \u00b6 If you have any questions or issues you can create a new [issue here][issues]. If you have created a sample that you would like to share, please do so - pull requests are very welcome! Ideally create a topic branch for every contribution or change you would like to suggest. For example: Fork the repo Create your feature branch ( git checkout -b my-new-feature ) Commit your changes ( git commit -am 'Added some feature' ) Push to the branch ( git push origin my-new-feature ) Create new Pull Request License \u00b6 If you would like to see the detailed LICENSE click here . # # Copyright 2020- IBM Inc. All rights reserved # SPDX-License-Identifier: Apache2.0 #","title":"Samples"},{"location":"ai-on-z-samples/#ai-on-z-samples","text":"","title":"ai-on-z-samples"},{"location":"ai-on-z-samples/#scope","text":"This repository contains small, useful examples that serve to demonstrate some of the interesting technologies available for use on IBM Z and LinuxONE systems.","title":"Scope"},{"location":"ai-on-z-samples/#samples-in-this-project","text":"TensorFlow Serving on zCX and z/OS simple app example. ONNX model export and conversion examples","title":"Samples in this project:"},{"location":"ai-on-z-samples/#notes","text":"If you have any questions or issues you can create a new [issue here][issues]. If you have created a sample that you would like to share, please do so - pull requests are very welcome! Ideally create a topic branch for every contribution or change you would like to suggest. For example: Fork the repo Create your feature branch ( git checkout -b my-new-feature ) Commit your changes ( git commit -am 'Added some feature' ) Push to the branch ( git push origin my-new-feature ) Create new Pull Request","title":"Notes"},{"location":"ai-on-z-samples/#license","text":"If you would like to see the detailed LICENSE click here . # # Copyright 2020- IBM Inc. All rights reserved # SPDX-License-Identifier: Apache2.0 #","title":"License"},{"location":"ai-on-z-samples/CONTRIBUTING/","text":"Contributing In General \u00b6 Our project welcomes external contributions. If you have an itch, please feel free to scratch it. To contribute code or documentation, please submit a pull request . A good way to familiarize yourself with the codebase and contribution process is to look for and tackle low-hanging fruit in the issue tracker . /* Copyright <holder> All Rights Reserved. SPDX-License-Identifier: Apache-2.0 */","title":"CONTRIBUTING"},{"location":"ai-on-z-samples/CONTRIBUTING/#contributing-in-general","text":"Our project welcomes external contributions. If you have an itch, please feel free to scratch it. To contribute code or documentation, please submit a pull request . A good way to familiarize yourself with the codebase and contribution process is to look for and tackle low-hanging fruit in the issue tracker . /* Copyright <holder> All Rights Reserved. SPDX-License-Identifier: Apache-2.0 */","title":"Contributing In General"},{"location":"ai-on-z-samples/MAINTAINERS/","text":"MAINTAINERS \u00b6 Andrew M. Sica - andrewsi@us.ibm.com Steve Lafalce - slafalce@us.ibm.com","title":"MAINTAINERS"},{"location":"ai-on-z-samples/MAINTAINERS/#maintainers","text":"Andrew M. Sica - andrewsi@us.ibm.com Steve Lafalce - slafalce@us.ibm.com","title":"MAINTAINERS"},{"location":"ai-on-z-samples/onnx-conversion/","text":"Exporting or converting a model to the ONNX format \u00b6 This project contains some simple jupyter notebook examples demonstrating ONNX model conversion. This includes: - Exporting a simple model from Pytorch to ONNX format. - Converting a tensorflow model to ONNX, using tensorflow-onnx . These examples will require you install needed packages. This is not done as part of the notebook. There are numerous additional examples and guidance available, not only for Pytorch and TensorFlow, but for other frameworks as well. This includes: - Building, exporting or converting a model here - tensorflow-onnx examples - pytorch guidance and examples On IBM Z and LinuxONE, you can run these models using ONNX-MLIR . For z/OS users, we recommend you try Watson Machine Learning for z/OS Trial edition, available here .","title":"Exporting or converting a model to the ONNX format"},{"location":"ai-on-z-samples/onnx-conversion/#exporting-or-converting-a-model-to-the-onnx-format","text":"This project contains some simple jupyter notebook examples demonstrating ONNX model conversion. This includes: - Exporting a simple model from Pytorch to ONNX format. - Converting a tensorflow model to ONNX, using tensorflow-onnx . These examples will require you install needed packages. This is not done as part of the notebook. There are numerous additional examples and guidance available, not only for Pytorch and TensorFlow, but for other frameworks as well. This includes: - Building, exporting or converting a model here - tensorflow-onnx examples - pytorch guidance and examples On IBM Z and LinuxONE, you can run these models using ONNX-MLIR . For z/OS users, we recommend you try Watson Machine Learning for z/OS Trial edition, available here .","title":"Exporting or converting a model to the ONNX format"},{"location":"ai-on-z-samples/tf-zcx-zos/","text":"Demonstrating interaction between z/OS and TensorFlow Serving \u00b6 This project intends to demonstrate a call to TensorFlow Serving using REST API from a simple z/OS Java app. The purpose is to create a simple and easily deployable scenario that can be used on z/OS to understand serving concepts. This model performns a matrix multiplication of a [1,5] input tensor by a [5,1] weights tensor, where all weights are defined with a value of 1. This results in an output tensor shape of [1,1]. For example, with this model an input tensor of [[1,2,3,4,5]] is multiplied by a weights tensor of [[1],[1],[1],[1],[1]], resulting in a value of [[15.]] Note that since this project is intended for deployment to z/OS, we are avoiding managing dependecies through sub-moduling or Maven/Ant. The intent is to quickly try this project out without installing additional software. Required Jar files can be pulled from the references below. Deploying TensorFlow Serving on z/OS Container Extensions \u00b6 TensorFlow Serving can be obtained from the IBM Container Image Repository or built from source for Linux on Z IBM Container Image Repository Source Details on the TFServing API can be found here: https://www.tensorflow.org/tfx/serving/api_rest To deploy the example model, you can follow this procedure: SFTP the TensorFlow saved model to zCX. This should be the model folder and all subfolders. The saved model can be found in the model subdirectory of this project. The notebook used to create the model is simplemm.ipynb Create a new docker volume. e.g., docker volume create tfmodels tfmodels is the volume name we create to use in subsequent steps Copy the model directory into the docker volume. One approach is to create a simple container using the volume to allow a docker cp: docker container create --name tfsimple -v tfmodels:/models simple_image docker cp <host_model_dir> tfsimple:/models tfsimple is the container name we create to facilitate the copy via docker CP. simple_image can be any base image, and it can be deleted after this copy. models is a directory we choose to copy the model into. Run the TFServing image: - docker run -d --rm -p 8507:8501 -v tfmodels:/models -e MODEL_NAME=<model_name> <image id> - 8501 is the default TFServing REST port. Here it is mapped to zCX port 8507. - model_name is the TensorFlow model name; this is commonly the directory name that holds the saved_model.pb file Updating and deploying the z/OS application \u00b6 First, deploy the project to your host system. This program is intended for Unix environments. Suggestion on how to get github projects to z/OS can be found here: https://github.com/IBM/IBM-Z-zOS The following Jar files must be in the project root directory; Apache Commons Logging: commons-logging-1.1.3.jar Apache HttpClient: httpclient-4.5.13.jar Apache Httpcore: httpcore-4.4.14.jar Json Processing API javax.json-1.0.jar We suggest using SFTP to transmit the jars to the z/OS environment. Note: If different versions are used, update the Makefile to reference the correect version. run make to build compile the .java files issue the following command to try a TensorFlow Serving request: make run HOSTIP=tf-serve-ip PORT=tf-serv-port MODEL_DIR=model-dir PAYLOAD=input-tensor tf-serve-ip is the IP address of the server or instance hosting TensorFlow Serving tf-serv-port is the TF Serving REST port model-dir is the model path, which should typically be /v1/models/simplemm for the sample model. input-tensor consists of 5 comma separated decimal values. For example, if we mapped the TFServing REST port to 8507, we would use something like this (changing the IP address to one in use by zCX): make run HOSTIP=127.0.0.1 PORT=8507 MODEL_DIR=/v1/models/simplemm PAYLOAD=1.0,2.0,3.0,4.0,5.0 In addition to verbose messages, the result for test would show: { \"predictions\": [[15.0] ] }","title":"Demonstrating interaction between z/OS and TensorFlow Serving"},{"location":"ai-on-z-samples/tf-zcx-zos/#demonstrating-interaction-between-zos-and-tensorflow-serving","text":"This project intends to demonstrate a call to TensorFlow Serving using REST API from a simple z/OS Java app. The purpose is to create a simple and easily deployable scenario that can be used on z/OS to understand serving concepts. This model performns a matrix multiplication of a [1,5] input tensor by a [5,1] weights tensor, where all weights are defined with a value of 1. This results in an output tensor shape of [1,1]. For example, with this model an input tensor of [[1,2,3,4,5]] is multiplied by a weights tensor of [[1],[1],[1],[1],[1]], resulting in a value of [[15.]] Note that since this project is intended for deployment to z/OS, we are avoiding managing dependecies through sub-moduling or Maven/Ant. The intent is to quickly try this project out without installing additional software. Required Jar files can be pulled from the references below.","title":"Demonstrating interaction between z/OS and TensorFlow Serving"},{"location":"ai-on-z-samples/tf-zcx-zos/#deploying-tensorflow-serving-on-zos-container-extensions","text":"TensorFlow Serving can be obtained from the IBM Container Image Repository or built from source for Linux on Z IBM Container Image Repository Source Details on the TFServing API can be found here: https://www.tensorflow.org/tfx/serving/api_rest To deploy the example model, you can follow this procedure: SFTP the TensorFlow saved model to zCX. This should be the model folder and all subfolders. The saved model can be found in the model subdirectory of this project. The notebook used to create the model is simplemm.ipynb Create a new docker volume. e.g., docker volume create tfmodels tfmodels is the volume name we create to use in subsequent steps Copy the model directory into the docker volume. One approach is to create a simple container using the volume to allow a docker cp: docker container create --name tfsimple -v tfmodels:/models simple_image docker cp <host_model_dir> tfsimple:/models tfsimple is the container name we create to facilitate the copy via docker CP. simple_image can be any base image, and it can be deleted after this copy. models is a directory we choose to copy the model into. Run the TFServing image: - docker run -d --rm -p 8507:8501 -v tfmodels:/models -e MODEL_NAME=<model_name> <image id> - 8501 is the default TFServing REST port. Here it is mapped to zCX port 8507. - model_name is the TensorFlow model name; this is commonly the directory name that holds the saved_model.pb file","title":"Deploying TensorFlow Serving on z/OS Container Extensions"},{"location":"ai-on-z-samples/tf-zcx-zos/#updating-and-deploying-the-zos-application","text":"First, deploy the project to your host system. This program is intended for Unix environments. Suggestion on how to get github projects to z/OS can be found here: https://github.com/IBM/IBM-Z-zOS The following Jar files must be in the project root directory; Apache Commons Logging: commons-logging-1.1.3.jar Apache HttpClient: httpclient-4.5.13.jar Apache Httpcore: httpcore-4.4.14.jar Json Processing API javax.json-1.0.jar We suggest using SFTP to transmit the jars to the z/OS environment. Note: If different versions are used, update the Makefile to reference the correect version. run make to build compile the .java files issue the following command to try a TensorFlow Serving request: make run HOSTIP=tf-serve-ip PORT=tf-serv-port MODEL_DIR=model-dir PAYLOAD=input-tensor tf-serve-ip is the IP address of the server or instance hosting TensorFlow Serving tf-serv-port is the TF Serving REST port model-dir is the model path, which should typically be /v1/models/simplemm for the sample model. input-tensor consists of 5 comma separated decimal values. For example, if we mapped the TFServing REST port to 8507, we would use something like this (changing the IP address to one in use by zCX): make run HOSTIP=127.0.0.1 PORT=8507 MODEL_DIR=/v1/models/simplemm PAYLOAD=1.0,2.0,3.0,4.0,5.0 In addition to verbose messages, the result for test would show: { \"predictions\": [[15.0] ] }","title":"Updating and deploying the z/OS application"}]}