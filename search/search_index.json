{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting started with AI on IBM Z and LinuxONE systems","text":"<p>IBM Z and LinuxONE feature state of the art hardware and software capabilities designed to optimize AI at scale alongside your enterprise's most critical workloads and data. </p> <p>AI on IBM Z and LinuxONE is a large umbrella that covers both server technology as well as numerous IBM and vendor offerings and open-source projects. This includes enhancements such as the IBM Telum on-chip inference accelerator used by the IBM Z and LinuxONE systems. </p> <p>This github page is designed to help you get started on your journey to leveraging AI in your IBM Z or LinxuONE environment. It features information on how to leverage the best of the open source ecosystem on IBM Z and LinuxONE, as well as pointers to community edition and freely available software that can help you quickly get started. It additionally provides example use cases, code samples, and other content to serve as a technical resource. </p> <p>Use the navigation bar on the left to select the most apporpriate topic. For other questions, reach out directly to aionz@us.ibm.com</p>"},{"location":"aitoolkitloz/","title":"AI Toolkit for IBM Z and LinuxONE","text":""},{"location":"aitoolkitloz/#ai-toolkit-for-ibm-z-and-linuxone","title":"AI Toolkit for IBM Z and LinuxONE","text":"<p>The AI Toolkit for IBM Z\u00ae and LinuxONE is a family of popular open-source AI frameworks with IBM Elite Support and adapted for IBM Z and LinuxONE hardware.</p> <p>This is a select set of containerized software that has been vetted by IBM. The pre-built images are available through the IBM Z and LinuxONE Container Registry. </p> <p>These images can be run in Linux on Z environents (including z/OS Container Extensions). The software is optimized for IBM Z and LinuxONE platforms, including the ability to leverage the IBM Telum Integrated Accelerator for AI.</p> <p>As of 2025, the AI Toolkit includes the following components:</p> <ul> <li>IBM Z Deep Learning Compiler</li> <li>IBM Snap ML </li> <li>PyTorch</li> <li>TensorFlow </li> <li>TensorFlow Serving</li> <li>Triton Inference Server</li> </ul>"},{"location":"aitoolkitloz/#learn-more","title":"Learn more","text":"<p>Learn more about this Elite support offering here</p>"},{"location":"codingAIU/","title":"Compiler and AI framework developer resources","text":""},{"location":"codingAIU/#developing-for-the-integrated-accelerator-for-ai","title":"Developing for the Integrated Accelerator for AI","text":"<p>Most users of the IBM z16 Integrated Accelerator for AI will be accessing it through frameworks and technologies like the IBM Deep Learning Compiler, TensorFlow, PyTorch, or Snap ML. The other pages on this site are focused on this type of usage. Information on using these can be found throughout these pages. </p> <p>For those that wish to develop or enhance AI frameworks and compilers to leverage the Integrated Accelerator for AI, a lower level SDK is needed. To facilitate this, IBM has created the zDNN library. IBM zDNN is the IBM Z Deep Neural Network library, which features C apis that can simplify use of the z16 accelerator.</p> <p>Details and examples can be found in the IBM zDNN github.</p> <p>Note that zDNN library is part of z/OS in all in-service releases, and is also available for RHEL, Ubuntu, SUSE at various releases. </p>"},{"location":"help/","title":"Getting help","text":""},{"location":"help/#resources-and-contacts","title":"Resources and Contacts","text":"<p>The path to getting started with AI on IBM Z and LinuxONE can vary based on the use case and goal that you have.</p> <p>In addition to this page, the following resources should be helpful:</p> <ul> <li>Journey to AI on IBM Z and LinuxONE content solution</li> <li>AI on IBM Z and LinuxONE Community</li> <li>Contact the IBM zSystems and LinuxONE AI core team</li> </ul>"},{"location":"infusing/","title":"Infusing AI into your IBM zSystem business applications","text":""},{"location":"infusing/#infusing-ai-into-ibm-z-and-linuxone-applications","title":"Infusing AI into IBM Z and LinuxONE applications","text":"<p>For detailed up to date information, see the Journey to AI on IBM Z and LinuxONE content solution</p> <p>See section \"Infusing AI into applications\" under \"Learn more\". </p>"},{"location":"onnxconv/","title":"ONNX model conversion","text":""},{"location":"onnxconv/#converting-models-to-the-onnx-format","title":"Converting models to the ONNX format","text":"<p>ONNX is the Open Neural Network eXchange. You can read more about it here and explore how it is leveraged by the IBM Z Deep Learning Compiler here.</p> <p>Converting or exporting trained deep learning models to the ONNX format is a requirement for leveraging the IBM Z Deep Learning Compiler, whether stand-alone or as part of Machine Learning for z/OS. </p> <p>The approach in deep learning frameworks varies - for example:</p> <ul> <li>TensorFlow models must be converted using the open-source tensorflow-onnx package.</li> <li>PyTorch models can be exported directly to ONNX using PyTorch APIs.</li> </ul> <p>Information on exporting models from other frameworks can be found on the ONNX community page.</p>"},{"location":"onnxconv/#best-practices","title":"Best practices","text":"<ul> <li> <p>To avoid endian issues, we strongly recommend converting models to the ONNX format on the platform on which they were trained. (e.g., if a model was trained on an x86 environment, convert the model to ONNX on an x86 environment before transmitting to IBM Z or LinuxONE). </p> </li> <li> <p>For Deep Learning Compiler usage: to determine which ONNX opset to specify for model conversion, see the following references:</p> <ul> <li>NNPA supported opset level</li> <li>CPU supported opset level</li> </ul> </li> <li> <p>On z16 or LinuxONE 4 machines (or later), we recommend using the highest opset level as specified under NNPA operator support. This will generally be the first statement in the NNPA supported opset level file linked above.</p> </li> </ul>"},{"location":"onnxconv/#determining-whether-model-operations-will-target-the-on-chip-ai-accelerator-featured-on-ibm-telum-i-and-telum-ii-processors","title":"Determining whether model operations will target the on-chip AI accelerator featured on IBM Telum I and Telum II processors:","text":"<p>When compiling with the IBM Z Deep Learning Compiler, you can get information and insights into various model operations and whether they will run on the accelerator. This can be critical information to help explore model performance. </p> <ul> <li>For guidance on displaying model operation targets (CPU or Z accelerator)</li> <li>Runtime instrumentation can be enabled</li> <li>Check here for other performance guidance</li> </ul>"},{"location":"onnxdlc/","title":"ONNX and the IBM Deep Learning Compiler","text":""},{"location":"onnxdlc/#deploying-onnx-deep-learning-models-on-ibm-z-or-linuxone-systems","title":"Deploying ONNX deep learning models on IBM Z or LinuxONE Systems","text":"<p>ONNX is the Open Neural Network eXchange. You can read more about it here.</p> <p>ONNX establishes a streamlined path to take a project from playground to production.  </p> <p>With ONNX, you can start a data science project using the frameworks and libraries of your choosing, including popular frameworks such as PyTorch and TensorFlow. The model can be developed and trained leveraging these frameworks on the training platform of your choice. Once the model is trained and ready to begin the deployment journey, you would export or convert it to the ONNX format. </p> <p>Tools such as Netron allow inspection and exploration of an ONNX model. When it comes to running the model, there are various back-ends that can be used to test and serve ONNX models. This includes model compilers such as the IBM Z Deep Learning Compiler (zDLC), which is based on ONNX-MLIR. </p> <p>The ONNX-MLIR project provides compiler technology to transform a valid Open Neural Network Exchange (ONNX) graph into code that implements the graph with minimum runtime support. It implements the ONNX standard and is based on the underlying LLVM/MLIR compiler technology. </p> <p>The result of this compiler is a lightweight shared object library that has no dependencies on the framework or libraries that the model was developed and trained in. It can be used for inference from C++, Java or Python programs. </p>"},{"location":"onnxdlc/#ibm-telum-integrated-accelerator-for-ai","title":"IBM Telum Integrated Accelerator for AI","text":"<p>IBM Research enhanced the IBM Z Deep Learning Compiler (zDLC) to target the IBM Integrated Accelerator for AI for a variety of ONNX primitives. This support has been contributed to ONNX-MLIR, which is the foundation for the IBM Deep Learning Compiler. ONNX-MLIR has been updated to support IBM z17's new Telum II processor.</p>"},{"location":"onnxdlc/#getting-started-with-the-ibm-z-deep-learning-compiler","title":"Getting Started with the IBM Z Deep Learning Compiler","text":"<p>The best approach to getting started with ONNX models using the IBM Z Deep Learning Compiler will depend on the IBM Z or LinuxONE operating system on which you plan to use the inference program. </p> <p>z/OS users can either choose a Machine Learning for z/OS (MLz) based approach or leverage Linux on Z options; in either case, z/OS Container Extensions will be required to utilize the IBM Z Deep Learning Compiler.</p> <ul> <li>Machine Learning for z/OS, which is a z/OS product that manages full model lifecycle and includes numerous features to improve performance for AI models and simplify deployment. <ul> <li>Enables you to upload your ONNX model then compile and deploy at the push of a button. </li> <li>Supports server side mini-batching for ONNX/DLC model serving to get the best benefit out of the Integrated Accelerator for AI.</li> <li>Infuse AI into z/OS applications either through native CICS, IMS, and Java native scoring services or through using model server REST endpoints.</li> </ul> </li> </ul> <p>Linux on Z, z/OS Container Extensions, and LinuxONE users can leverage the Deep Learning Compiler directly to create model programs that can be incorporated into serving environments or applications directly.</p> <ul> <li>Available through the IBM Z and LinuxONE Container Registry listed under zDLC.</li> <li>IBM zDLC official product page.</li> <li>Command-line model compiler that produces a .so library with optional Java and Python wrappers.</li> <li>These model libraries can be leveraged either directly or through Triton Inference Server, which is part of the AI Toolkit for IBM Z And LinuxONE. <ul> <li>Leveraging DLC with Triton Inference Server on LoZ</li> </ul> </li> <li>Additional samples: <ul> <li>Documentation and samples, including C++, Java, and Python clients</li> </ul> </li> </ul> <p>Read our blogs on ONNX for more information:</p> <ul> <li>IBM Z and the Open Neural Network Exchange</li> <li>Leveraging ONNX models on IBM Z and LinuxONE</li> <li>Announcing ONNX-MLIR/IBM Deep Learning Compiler for Linux on Z and LinuxONE</li> <li>Announcing ONNX-MLIR/IBM Z Deep Learning Compiler Telum II support</li> </ul>"},{"location":"opensource/","title":"Obtaining open-source packages","text":""},{"location":"opensource/#obtaining-open-source-ai-packages-on-ibm-z-and-linuxone","title":"Obtaining open source AI packages on IBM Z and LinuxONE","text":"<p>You can obtain open source AI packages using many of the methods you are familiar with.  </p> <p>Packages can be installed using Linux packages managers, PyPI, or may be built from source where necessary.</p> <p>Resources: </p> <ul> <li>AI Toolkit for IBM Z and LinuxONE</li> <li>Python AI Toolkit for IBM z/OS</li> <li>IBM Z and LinuxONE Container Registry</li> </ul> <p>Additional guidance on building specific packages for s390x can be found on the Linux on Z ecosystem repository</p> <p>If you need assistance with specific other packages, contact us. </p>"},{"location":"pmmlconv/","title":"PMML model conversion","text":""},{"location":"pmmlconv/#converting-models-to-the-pmml-format","title":"Converting models to the PMML format","text":"<p>PMML is the predictive model markup language. You can read more about it here. Machine learning models converted to the PMML format can be leveraged by IBM Snap ML; as described here, tree based models can utilize the IBM z16 Integrated Accelerator for AI.  </p> <p>Additionally,  Machine Learning for z/OS can deploy PMML models. </p> <p>Generally, the best approach to converting models to PMML is to use sklearn2pmml. This allows the user to create a PMMLPipeline object, which can contain either scikit-learn, xgboost, or lightGBM models as part of a pipeline. </p> <p>For considerations for using PMML models in  Machine Learning for z/OS, see the MLz documentation</p>"},{"location":"pmmlconv/#samples","title":"Samples","text":"<p>For an example of converting a scikit-learn model to PMML, see the IBM Snap ML examples; one such example can be found here</p> <p>Additionally, here is a brief code snippet showing the process: </p> <pre><code># Create a scikit-learn Random Forest Classifier model\nmodel = RandomForestClassifier(n_estimators = 200, max_depth=6, n_jobs=4, random_state=42)\n\n# Train a PMML pipeline that uses the scikit-learn model defined above\npipeline = PMMLPipeline([(\"model\", model)]).fit(X_train, y_train)\n\n# Save the trained PMML pipeline to a file, e.g., \"model.pmml\"\nsklearn2pmml(pipeline, \"model.pmml\", with_repr=True)\n</code></pre>"},{"location":"pyaitoolkit/","title":"Python AI Toolkit for IBM z/OS","text":""},{"location":"pyaitoolkit/#python-ai-toolkit-for-ibm-zos","title":"Python AI Toolkit for IBM z/OS","text":"<p>Python AI Toolkit for IBM z/OS is a library of relevant open source software to support today's artificial intelligence (AI) and machine learning (ML) workloads. It is a collection of Python packages that can be installed and managed using Package Installer for Python (pip), the common Python package manager. These packages are provided to pip from an IBM-hosted PyPi-style repository, leveraging supply chain security, that makes your software management experience common across your Python environments.</p> <p>IBM support is available for this product through IBM Shopz.</p>"},{"location":"pyaitoolkit/#getting-started-with-the-python-ai-toolkit-for-ibm-zos","title":"Getting Started with the Python AI Toolkit for IBM z/OS","text":"<p>Python AI Toolkit for IBM z/OS is available within the IBM z/OS environment.</p> <p>Visit the getting started tab on the Python AI Toolkit for IBM z/OS product page to view all the latest documentation installing and using the product.</p>"},{"location":"pyaitoolkit/#resources","title":"Resources","text":"<p>For more information, check out these helpful resources:</p> <ul> <li>Overview Video</li> <li>Getting Started Video</li> <li>Redpaper Solution Guide</li> <li>FAQ</li> <li>Annoucement Letter</li> <li>Journey to Open Data Analytics</li> <li>IBM Open Enterprise SDK for Python</li> </ul>"},{"location":"pytorch/","title":"PyTorch","text":""},{"location":"pytorch/#ibm-z-accelerated-for-pytorch","title":"IBM Z Accelerated for PyTorch","text":"<p>PyTorch is an open source machine learning framework that provides a flexible platform for building deep learning models. Released by Facebook's AI Research lab in 2016, PyTorch allows developers to create and modify models easily through its dynamic structure, which offers immediate feedback. This adaptability makes it particularly appealing for researchers and developers who want to experiment with new ideas.</p> <p>PyTorch can be used on IBM zSystems and LinuxONE on Linux environments (including z/OS Container Extensions).</p> <p>IBM Z Accelerated for PyTorch can be leveraged for various use cases, including applications such as language processing and computer vision. It is optimized to exploit the acceleration of IBM Z Integrated Accelerator for AI on IBM z16 and LinuxONE 4, which improves inference performance for a wide range of PyTorch models. </p>"},{"location":"pytorch/#getting-started-with-the-ai-toolkit-for-ibm-z-and-linuxone","title":"Getting started with the AI Toolkit for IBM Z and LinuxONE","text":"<p>The AI Toolkit for IBM Z and IBM\u00ae LinuxONE is designed to enable our clients to deploy and accelerate the adoption of popular open source AI frameworks on their z/OS\u00ae and IBM\u00ae LinuxONE platforms. The AI Toolkit follows a rigorous IBM Secure Engineering process that vets and scans open source AI-serving frameworks and IBM-certified containers for security vulnerabilities and validates compliance with industry regulations. Clients can also purchase IBM Elite Support for AI Toolkit for IBM Z and LinuxONE.</p> <p>The AI Toolkit components are availble in the IBM Z and LinuxONE Container Image Registry (ICR). This registry includes open-source software in container images that are often used as the foundation for new composite workloads. It provides a secure and trustworthy content source. On the IBM Z and LinuxONE Container Registry, these components of AI Toolkit for IBM Z and LinuxONE are freely available. </p> <p>See also:  Accelerating PyTorch Inference on IBM Z and LinuxONE AI Toolkit page for IBM Z Accelerated for PyTorch</p>"},{"location":"resources/","title":"Need access to a IBM Z or LinuxONE Environment?","text":""},{"location":"resources/#getting-access-to-ibm-z-and-linuxone-environments","title":"Getting access to IBM Z and LinuxONE environments.","text":"<p>Various Linux on Z VM and CI enviroment resources can be found here: IBM developer blog on resources</p>"},{"location":"snapml/","title":"IBM Snap Machine Learning (Snap ML)","text":""},{"location":"snapml/#traditional-machine-learning-with-ibm-snap-ml","title":"Traditional Machine Learning with IBM Snap ML","text":"<p>Snap ML is a library that provides high speed training and inference of popular machine learning models. You can read more about it here.</p> <p>Standard machine learning models power most of today's machine learning applications in business and are very popular among practitioners as well. Snap ML has been designed to address some of the biggest challenges that companies and practitioners face when applying machine learning to real use cases.</p> <p>Snap ML is a library for accelerating the training and inference of popular Machine Learning (ML) models:</p> <ul> <li>Provides high-performance implementations of:</li> <li>Generalized Linear Models</li> <li>Tree-based Models</li> <li>Gradient Boosting Machines</li> </ul> <p>Addresses ML needs of Data Scientists by being:</p> <ul> <li>Fast</li> <li>Resource-efficient</li> <li>Accurate</li> <li>Scalable to TB-scale datasets</li> </ul> <p>Snap ML:</p> <ul> <li>Is developed &amp; maintained by IBM Research. </li> <li>Is fully compatible with the scikit-learn Python API.</li> <li>Supports accelerated scoring of scikit-learn, XGBoost and LightGBM trained models when exported or converted to: PMML, JSON, ONNX.</li> </ul>"},{"location":"snapml/#ibm-telum-integrated-accelerator-for-ai","title":"IBM Telum Integrated Accelerator for AI","text":"<p>Starting with Snap ML version 1.9.0, Snap ML can utilize the IBM Integrated Accelerator for AI. </p> <p>Integrated AI Accelerator exploitation can be enabled at model import time for:</p> <ul> <li>Random Forest</li> <li>Extra Trees</li> <li>Gradient Boosting Machines</li> </ul>"},{"location":"snapml/#getting-started-with-snap-ml","title":"Getting started with Snap ML","text":"<p>IBM Snap ML is available as part of Machine Learning for z/OS, as well as for IBM LinuxONE and Linux on Z environments - including z/OS container extensions. A prebuilt container is available as part of the IBM AI Toolkit Offering; additionally Snap ML can be installed via PyPI, and Snap ML is a component of Cloud Pak for Data on Linux on Z.</p> <p>As mentioned, IBM Snap ML component of the AI Tookit for IBM Z and LinuxONE. You can find the latest container images in the IBM Z and LinuxONE Container Image registry under ibmz-accelerated-for-snapml.</p> <p>Additionally, an s390x (Linux on IBM Z package) is available via PyPI: <pre><code>pip install snapml \n</code></pre> - This enables install into a python environment through standard mechanisms, at no charge. </p> <p>Snap ML is additionally an available python framework in IBM Cloud Pak for Data.  </p> <p>Other assets include: </p> <ul> <li>Click here for official documentation</li> <li>Click here for examples using Snap ML</li> <li>Leveraging Snap ML with Triton Inference Server on LoZ</li> <li>Example using Snap ML with BentoML Serving</li> </ul>"},{"location":"snapml/#notes","title":"Notes:","text":"<ul> <li>Starting With Snap ML 1.9.1, SnapML-trained tree-based models (RF, ET and GB) can be exported to PMML.</li> </ul>"},{"location":"solutiontemplates/","title":"AI Solution Templates","text":"<p>AI Solution Templates on IBM Z &amp; LinuxONE are available within The Open Mainframe Project!</p>"},{"location":"solutiontemplates/#what-are-they","title":"What are they?","text":"<p>AI Solution Templates are a suite of pre-built blueprints that guide you through the full AI lifecycle on IBM Z with various enterprise use cases while leveraging a variety of technologies. Whether you\u2019re a senior data scientist or have no previous AI skills, build your own AI model, deploy it on IBM Z, and integrate it into a business application.</p> <p>You can now experience the following AI Solution Templates:</p> <ul> <li>Fraud Detection using MLz</li> <li>Credit Risk Assessment using MLz</li> <li>Health Insurance Claims using MLz</li> <li>Fraud Detection using AI Toolkit</li> </ul>"},{"location":"solutiontemplates/#why-should-i-use-them","title":"Why should I use them?","text":"<p>AI Solution Templates accelerate your time to value by jumpstarting your AI deployment on IBM Z through rapid prototyping. They include a guided experience for each use case with sample reference synthetic datasets, AI model training pipelines, sample application code, and more. Deploy your AI models on IBM Z and benefit from the IBM Z Integrated Accelerator for AI.</p>"},{"location":"solutiontemplates/#how-to-get-started","title":"How to get started?","text":"<p>AI Solution Templates are available within the Open Mainframe Project at the Linux Foundation inside of the Ambitus project. You can find all the latest AI Solution Templates from our landing page. Detailed instructions on how to get started are included within each AI Solution Template.</p> <p>Reach out to engage with our AI on IBM Z team (aionz@us.ibm.com) if you\u2019re interested in a hands-on workshop.</p>"},{"location":"tensorflow/","title":"TensorFlow and TensorFlow Serving","text":""},{"location":"tensorflow/#tensorflow-on-ibm-zsystems-and-linuxone","title":"TensorFlow on IBM zSystems and LinuxONE","text":"<p>TensorFlow is a open source machine learning framework. It has a comphrensive set of tools that enable model development, training and inference. It also features a rich, robust ecosystem. </p> <p>TensorFlow can be used on IBM Z and Linux ONE on Linux environments - including z/OS Container Extensions.</p> <p>With TensorFlow Servng, you can expose REST and GRPC endpoints for model inference requests. This supports high throughput, low latency inference serving.</p> <p>On IBM zSystems and LinuxONE, TensorFlow is built to exploit the vector architecture for training and inference operations. On IBM z16 hardware, TensorFlow can now leverage new acceleration capabilities enabled by the Telum I and Telum II processors. Read more below!</p>"},{"location":"tensorflow/#ibm-telum-integrated-accelerator-for-ai","title":"IBM Telum Integrated Accelerator for AI","text":"<p>On IBM z16, LinuxONE 4, and later machines (running Linux on IBM Z or IBM\u00ae z/OS\u00ae Container Extensions (IBM zCX)), TensorFlow core Graph Execution will leverage new inference acceleration capabilities that transparently target the IBM Integrated Accelerator for AI through the IBM z Deep Neural Network (zDNN) library. The IBM zDNN library contains a set of primitives that support Deep Neural Networks. These primitives transparently target the IBM Integrated Accelerator for AI on IBM z16 and later. No changes to the original model are needed to take advantage of the new inference acceleration capabilities.</p>"},{"location":"tensorflow/#getting-started-with-the-ai-toolkit-for-ibm-z-and-linuxone","title":"Getting started with the AI Toolkit for IBM Z and LinuxONE","text":"<p>The recommended way to obtain the TensorFlow and TensorFlow Serving components of the AI Toolkit for IBM Z and LinuxONE is to download prebuilt container images. Instructions for downloading them are available at the following links:</p> <ul> <li>TensorFlow</li> <li>TensorFlow Serving</li> </ul>"},{"location":"terminology/","title":"Terminology","text":""},{"location":"terminology/#terminology-and-technology-relationships","title":"Terminology and technology relationships","text":"<p>This page is focused primarily on IBM technologies and terminology. General open source and AI terminology can be found elsewhere on the net. </p> <p>Where applicable, for links see the sidebar.</p> <ul> <li> <p>IBM Z Deep Learning Compiler is an ONNX model compiler based on ONNX-MLIR. When compiling for IBM z16, it uses IBM zDNN APIs for ONNX operators that the Integrated Accelerator for AI supports. </p> </li> <li> <p>IBM Z Integrated Accelerator for AI is an on-chip AI inference accelerator that optimizes and acclerates complex AI operations beyond just matrix multiplication. It is invoked through the Neural Network Processing Assist (NNPA) instruction.</p> </li> <li> <p>NNPA (Neural Network Processing Assist) is a new architected instruction with the IBM z16 which is used to execute work on the IBM Integrated Accelerator for AI. Framework and compiler developers would typically use IBM zDNN library instead of attempting to directly code the NNPA instruction. Note that this is not something end users need to worry about - frameworks and compilers like the IBM Deep Learning Compiler (ONNX-MLIR), Snap ML, TensorFlow will handle this for you (at the right software levels!).</p> </li> <li> <p>IBM Telum is the zArchitecture chip that is featured in the IBM z16. IBM Telum includes an on-chip inference accelerator, known as the IBM Integrated Accelerator for AI.</p> </li> <li> <p>Machine Learning for z/OS is a full lifecycle AI solution that features the ability to deploy AI models directly to z/OS for infusion into z/OS applications.</p> </li> <li> <p>IBM zDNN is IBM's open source API library for utilizing the IBM Integrated Accelerator for AI. It provides C APIs that greatly simplify use of the accelerator, including 'helper' functions. zDNN invokes the NNPA instruction on behalf of the caller to drive the accelerator.</p> </li> <li> <p>AI Toolkit for IBM Z and LinuxONE provides IBM Elite Support for a family of popular open-source AI frameworks. The assets are available at no-charge; the offering is focused on providing support. </p> </li> </ul>"},{"location":"tritonis/","title":"Triton Inference Server","text":""},{"location":"tritonis/#triton-inference-server-for-linux-on-z-environments","title":"Triton Inference Server for Linux on Z environments","text":"<p>Triton Inference Server is a model server open-sourced by Nvidia. Triton supports model inference on both CPU and GPU devices and is commonly used across a wide variety of platforms and architectures, including s390x (Linux on Z). On Linux on Z, Triton is able to leverage AI frameworks that can take advantage of both the SIMD architecture as well as the IBM Integrated Accelerator for AI. </p> <p>Features of Triton include:</p> <ul> <li>Server-side micro-batching (Dynamic Batching)</li> <li>Support for multiple frameworks</li> <li>Supports customization: new frameworks, rule integrations.</li> <li>Model version control</li> <li>Concurrent model execution</li> <li>Metrics/Monitoring Integration</li> </ul> <p>Triton Inference Server is quite flexible and supports a wide variety of model types. It also has the ability to create custom model backends, which make it extremely flexible for a variety of scenarios.  In our testing, we have focused on two primary paths which allow us to deploy models that can leverage the Integrated Accelerator for AI when deployed on Telum I  and  Telum II processors. </p> <p>These are:</p> <ul> <li>Traditional machine learning models in the PMML, ONNX, or JSON format that are run using an IBM Snap ML C++ runtime.</li> <li>Deep Learning models in the ONNX model format and compiled with the IBM Deep Learning Compiler.</li> <li>Recently introduced PyTorch backend with Telum II support brings unique value by deploying real world multi-model AI use cases on Telum II.</li> </ul> <p>You can easily build and experiment with either of these capabilities on your Linux on Z environment. </p>"},{"location":"tritonis/#getting-started-with-the-ai-toolkit-for-ibm-z-and-linuxone","title":"Getting started with the AI Toolkit for IBM Z and LinuxONE","text":"<p>Triton Inference Server is a component of the AI Tookit for IBM Z and LinuxONE. You can find the latest container images in the IBM Z and LinuxONE Container Image registry under ibmz-accelerated-for-nvidia-triton-inference-server. </p> <p>IBM has made detailed examples available that can be used to try the Snap ML support: https://github.com/IBM/ai-on-z-triton-is-examples/tree/main/snapml-examples.</p> <p>Additionally as mentioned Triton can also be used to deploy ONNX models compiled with the IBM Deep Learning Compiler. Guidance on building and using the Triton Deep Learning Compiler backend can be found here: https://github.com/IBM/onnxmlir-triton-backend</p> <p>Note that IBM Snap ML is available for install with PyPI , while the IBM Deep Learning Compiler is available via the IBM Z and LinuxONE Container Repository. Both are no charge sources. </p>"},{"location":"zAccel/","title":"Leveraging the IBM Telum Integrated Accelerator for AI","text":""},{"location":"zAccel/#leveraging-the-ibm-integrated-accelerator-for-ai","title":"Leveraging the IBM Integrated Accelerator for AI","text":"<p>The IBM Integrated Accelerator for AI is an on-chip AI accelerator available on the IBM Telum chip that is part of IBM z16 and LinuxONE 4 servers. It is designed to enable high throughput, low latency inference for deep learning and machine learning. </p> <p>With IBM z16 and the Integrated Accelerator for AI, you can build and train your models on any platform - including IBM zSystems and LinuxONE. When you are ready to deploy your assets, they will receive transparent acceleration and optimization on IBM zSystems, and will leverage the best available acceleration for the model type.</p> <p>The IBM Integrated Accelerator for AI is more than just a matrix multiply accelerator - it provides optimization and acceleration for a wide set of complex functions commonly found in deep learning and machine learning models. This enables a broader set of functions to be accelerated on the chip.</p> <p>The following operations are supported on the accelerator (by machine generation):</p> Operation z16 or LinuxONE 4 (Telum I) z17 (Telum II) LSTM Activation Supported Supported GRU Activation Supported Supported Fused Matrix Multiply, Bias op Supported + added transpose, INT8 quantization Fused Matrix Multiply (w/ broadcast) Supported + added transpose, INT8 quantization Batch Normalization Supported Supported Fused Convolution, Bias Add, Relu Supported Supported L2 Norm Supported Layer Normalization Supported Max Pool 2D Supported Supported Average Pool 2D Supported Supported Softmax Supported Supported Relu Supported Supported Leaky Relu Supported Gelu Supported Tanh Supported Supported Sigmoid Supported Supported Add Supported Supported Subtract Supported Supported Multiply Supported Supported Divide Supported Supported Min Supported Supported Max Supported Supported Log Supported Supported Square root Supported Transform (Tensor) Supported Reduce Supported <p>These allow supporting frameworks to target a significantly larger set of operations to the Integrated Accelerator for AI. </p>"},{"location":"zAccel/#using-the-integrated-accelerator-for-ai","title":"Using the Integrated Accelerator for AI","text":"<p>Depending on your model type, there are a few essential approaches to leveraging the Integrated Accelerator for AI. These capabilities are all available in various IBM product offerings as well as through no-cost channels (such as the IBM Z Container Image Repository). </p> <p>For deep learning models, such as those created in PyTorch or TensorFlow:</p> <ul> <li>ONNX deep learning models, when compiled using the IBM Z Deep Learning Compiler (onnx-mlir). </li> <li>TensorFlow </li> <li>PyTorch</li> </ul> <p>For machine learning models, such as those created in sci-kit learn, XGBoost, or lightGBM:</p> <ul> <li>IBM Snap ML, a machine learning framework that provides optimized training and inference.</li> </ul> <p>For those interested in enhancing frameworks or compilers to use the Integrated Accelerator for AI:</p> <ul> <li>IBM zDNN<ul> <li>This is the accelerator development library, which is intended for use by those interested in enhancing frameworks or compilers to use the accelerator.  </li> </ul> </li> </ul> <p>For further details, use the navigation bar on this page to select a 'Featured Frameworks and Technologies' choice. </p> <p>Each of these are available as standalone packages, free of charge, or embedded within IBM products such as  Machine Learning for z/OS and Cloud Pak for Data. </p> <p>Further reading:</p> <ul> <li>IBM Telum announcement</li> </ul>"},{"location":"blog/","title":"Updates and news","text":""},{"location":"blog/2025/04/08/announcing-the-ibm-z17/","title":"Announcing the IBM z17","text":"<ul> <li>Further details available here</li> </ul>"},{"location":"blog/2024/03/15/launch-of-ai-solution-templates-on-ibm-z--linuxone/","title":"Launch of AI Solution Templates on IBM Z &amp; LinuxONE","text":"<ul> <li>A guided hands-on experience to jumpstart your deployment of AI on IBM Z</li> <li>Further details available here</li> </ul>"},{"location":"blog/2024/01/12/launch-of-ai-toolkit-for-ibm-z-and-linuxone/","title":"Launch of AI Toolkit for IBM Z and LinuxONE","text":"<ul> <li>provides IBM Elite support for popular open-source packages</li> <li>Further details available here</li> </ul>"},{"location":"blog/2024/01/12/launch-of-ai-toolkit-for-ibm-z-and-linuxone/#updates-to-our-101-pages","title":"Updates to our 101 pages","text":"<p>We've added quite a bit of new content - including:</p> <ul> <li>WMLz renamed as Machine Learning for z/OS</li> <li>Added additional ONNX model guidance</li> <li>Various updates to existing sections</li> </ul>"},{"location":"blog/2024/06/20/new-linux-on-z-focused-redbook-asset/","title":"New Linux on Z Focused redbook asset","text":"<ul> <li>A very deep exploration of AI on Linux on Z titled Enriching Linux on IBM Z Workloads with AI on the IBM Redbook site here</li> </ul>"},{"location":"blog/2024/11/22/updates-to-our-101-pages/","title":"Updates to our 101 pages","text":"<p>With the GA of PyTorch as a component of the AI Toolkit, pages added: - Now available - IBM Z Accelerated for PyTorch</p>"},{"location":"blog/2023/07/07/updates-to-our-101-pages/","title":"Updates to our 101 pages","text":"<p>We've added quite a bit of new content - including:</p> <ul> <li>sections for guidance on converting models to ONNX and PMML</li> <li>information on using Triton inference server</li> <li>various updates to existing sections</li> </ul>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/category/general/","title":"general","text":""},{"location":"blog/category/ibm-z/","title":"IBM Z","text":""},{"location":"blog/category/site-updates/","title":"site updates","text":""},{"location":"blog/category/documentation/","title":"documentation","text":""}]}